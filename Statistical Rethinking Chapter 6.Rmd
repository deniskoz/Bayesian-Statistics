#6E1. State the three motivating criteria that define information entropy. Try to express each in your own words.
1. The variable is continous.
2. If there are more variables to predict, the level of uncertainty will increase.
3. To measure the uncertainty of multiple variables, a sum of the variable's individual uncertainties is possible.

#6E2. Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?
```{r}
p <- c( 0.3 , 0.7 )
-sum( p*log(p) )

```


#6E3. Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, ”3” 25%, and ”4” 30% of the time. What is the entropy of this die?
```{r}
p <- c( 0.2 , 0.25, 0.25, 0.30 )
-sum( p*log(p) )
```


#6E4. Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?
```{r}
p <- c( 1/3 , 1/3, 1/3)
-sum( p*log(p) )
```


#6M1. Write down and compare the definitions of AIC, DIC, and WAIC. Which of these criteria is most general? Which assumptions are required to transform a more general criterion into a less general one?

AIC

Is reliable when:
  (1) The priors are flat or overwhelmed by the likelihood.
  (2) The posterior distribution is approximately multivariate Gaussian.
  (3) The sample size N is much greater than the number of parameters k.

DIC

  (1) Is Like AIC, but is aware of informative priors.
  (2) DIC is calculated from the posterior distribution of the training deviance.

WAIC

  (1) Calculated by taking averages of log likelihood over the posterior distribution.
  (2) An estimate of out-of-sample deviance and does not require a multivariate Gaussian posterior.
  (3) it is pointwise. This means that uncertainty in prediction is considered case-by-case, or point-by-point, in the data.

Most General:
WAIC, DIC, AIC.

Assumptions:
When posterior predictive mean is a good representation of the posterior predictive distribution, WAIC and DIC will tend to agree.

When priors are effectively flat or overwhelmed by the amount of data, the DIC and AIC will tend to agree.


#6M2. Explain the difference between model selection and model averaging. What information is lost under model selection? What information is lost under model averaging
Model selection is choosing one or several models with the lowest AIC/DIC/WAIC value and getting rid of the other models.
Model averaging is computing weights for the predictions of several models based on DIC/WAIC.

With model selection, accuracy of the models based on the differences in the information criteria is lost. Therefore, information is lost  on how confident we can be about the models selected.

With model averaging, some of the raw information criteria values of each model is lost.


#6M3. When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.
How else would it be possible to compare models? If there are a different amount of observations, the information criteria, deviance, and accuracy of the models will be different and not comparable. The models with fewer observations will "de facto" have a lower information criterion or be "better" and the the models with more observations will have a higher information criterion.

In addition it is important to remember that deviance is not divided by the number of cases, to make it an average, but rather just summed up across all cases. Therefore it is scaled by the amount of observations. Deviance is used for model accuracy.

#6M4. What happens to the effective number of parameters,as measured by DIC or WAIC,as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.
When a prior becomes more concentrated, the number of effective parameters goes down when measured by DIC or WAIC. Because, the model is less flexible.


#6M5. Provide an informal explanation of why informative priors reduce overfitting.

Informative priors adjust model parameters to giving enough information about the model but do not try to explain too much.


#6M6. Provide an information explanation of why overly informative priors result in underfitting.
If there are overly informative priors, samples from a model will not give no information and adding new data will not make a difference
